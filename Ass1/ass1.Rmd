---
title: "Assignment 1"
author: "Martynas Vaznonois, Andrei Puchkov, Carlo Peron (Group 1)"
date: "23 February 2024"
output: pdf_document
highlight: tango
editor_options: 
  markdown: 
    wrap: 72
---

`r options(digits=3)` `r options(warn=-1)`
`r data <- read.csv("Ice_cream-1.csv")`

## Exercise 1

**a)** The histogram and the qq plot look fairly indicative of normality
but the Shapiro-Wilk test provides evidence for the contrary.

```{r, fig.height=3, fig.width=6}
par(mfrow=c(1, 2)); qqnorm(data$video)
hist(data$video, freq=T, main="Histogram of video", xlab="Video")
shapiro.test(data$video)
```

Assuming normality, especially with a sample size $n>30$, we use the
z-score instead of the $t_{n-1}$-score to compute the 97% confidence
interval. Rearranging the formula, we find the number of participants
needed to make the CI at most 3. Finally, we compute the bootstrap CI by
simulating sampling from the same distribution and calculating the mean
test statistics.

```{r}
# CI
alpha = 0.03
ci <- qnorm(1 - alpha/2) * sd(data$video) / sqrt(length(data$video))
c(mean(data$video) - ci, mean(data$video) + ci)

# Participants
n <- (qnorm(1 - alpha/2) * sd(data$video) / (3/2))^2; ceiling(n)

# Bootstrap CI
B = 100000
Tstar = numeric(B)
for(i in 1:B)
  Tstar[i] = mean(sample(data$video, replace=T))
c(2*mean(Tstar) - quantile(Tstar, 1 - alpha/2), 2*mean(Tstar) - quantile(Tstar, alpha/2))
```

**b)** The first test below shows that the $H_0$ is rejected
($p<0.001$). The confidence interval for the true mean is
$(50.69; \infty)$. This shows that the true mean can be found within that
interval with 95% certainty. Because the mean of 50 under $H_0$ is not
in that interval, the probability of $H_0$ being true is less than 5%,
leading to its rejection. Under the second test, the $\mu_0$ is within
that same interval, which is why the p-value is above 0.05 and why $H_0$
cannot be rejected.

```{r}
t.test(data$video, mu=50, alternative="greater")
t.test(data$video, mu=51, alternative="greater")
```

**c)** The first test below is the sign (binomial) test. It assumes
$H_0: m=50$ to be the median and if the $H_0$ is correct, then there
should be approximately the same number of observations to the right and
left of it. The Wilcoxon test also considers ranks to make stronger
predictions. Comparing to **b)**, the t-tests require the data to be
normally distributed which is unclear in our case. The two latter tests
do not make such an assumption which may make them more applicable. On
the other hand, they throw away more information, and in the case of the
sign test, so much information has been lost that $H_0$ cannot be
rejected, like it was with the t-test. The Wilcoxon test only assumes
symmetrical distribution, which seems likely. And it, like the t-test,
provides enough evidence to reject $H_0$

```{r}
binom.test(sum(data$video>50), length(data$video), alternative='g')
wilcox.test(data$video, mu=50, alternative='g')
```

Testing whether less than 25% of the data falls below 42 comes down to
calculating the fraction of the data below 42 and checking if it is less
than 0.25.

```{r}
sum(data$video<42)/length(data$video) <= 0.25
```

**d)** The bootstrap test allows us to check if a particular sample is expected from some distribution. By calculating some statistic $T$ from many samples of some distribution $F$, we can estimate the likelihood of that statistic appearing in the distribution. Then, if that statistic of our sample $T(X_1,...,X_N)$ is unexpected compared to the simulated statistics, we can conclude that our sample did not come from distribution $F$. This can been seen in the code below which tests 101 normal distributions with different means to test which ones were plausible to yield our sample.

```{r}
# Bootstrap test
B = 10000; t = min(data$video); Tstar = numeric(B); means = NULL
for(m in 0:100){
  for(i in 1:B)
    Tstar[i] = min(rnorm(length(data$video), mean=m, sd=10))
  if(2*min(sum(Tstar<t)/B, sum(Tstar>t)/B) > 0.05)
    means = c(means, m)
}
range(means)
```

The Kolmogorov-Smirnov test can show whether two distributions are the same. It can estimate the distribution of our sample by computing the empirical distribution function and compare it to some distribution $F$. Therefore it is also applicable in this situation. We run it once to test the integer values of possible Gaussians for our data and then again to get a more accurate range.

```{r}
# Kolmogorov-Smirnov test, to roughly estimate the means which fit
means = NULL
for(m in 0:100){
  if(ks.test(data$video, pnorm, m, 10)[[2]] > 0.05)
    means = c(means, m)
}
range(means)
```

```{r}
# More precise KS test
means = NULL
for(m in seq(51, 54, by=0.001)){
  if(ks.test(data$video, pnorm, m, 10)[[2]] > 0.05)
    means = c(means, m)
}
range(means)
```

**e)** Here we create separate vectors for male scores and female ones. Then we run the 3 tests on the data to check if male scores are indeed higher than female. All tests provide support for the expert's claim.

```{r}
fvideo = data$video[data$female == 1]; mvideo = data$video[data$female == 0]

t.test(mvideo, fvideo, alternative='g')
wilcox.test(mvideo, fvideo, alternative='g')
ks.test(mvideo, fvideo, alternative='l')
```

To confirm the tests' applicability, we need to check the distribution of data. For t-tests, it needs to follow a normal distribution. The histograms and qq plots, as well as the Shapiro-Wilk tests, likely confirm this. We do not show it here, but the boxplots also indicate normality. Therefore, the t-test is applicable. The Wilcoxon rank sum test only assumes symmetry which also seems likely. The Kolmogorov-Smirnov test does not assume a particular distribution and so is also applicable. The permutation test on the other hand is not applicable because these are independent samples.

```{r, fig.height=6, fig.width=6}
par(mfrow=c(2, 2)); qqnorm(fvideo); qqnorm(mvideo); hist(fvideo); hist(mvideo)
shapiro.test(fvideo); shapiro.test(mvideo)
```

**f)** Here we first apply Pearson's correlation test which rejects $H_0: r=0$. This test assumes normality so we tested for it as in **e)**, and found that the data is not normal. Then we ran the Spearman's correlation test,
which does not assume normality and found that the correlation was still there, meaning that $H_0: r=0$ can be rejected.

```{r}
cor.test(data$video, data$puzzle)
cor.test(data$video, data$puzzle, method='spearman')
```

To test the hypothesis that the score on puzzles is higher than on video games, we ran a paired t-test. this found no difference. Unfortunately, `data$puzzle - data$video` is not normally distributed so a different test is needed. We then tried the Wilcoxon paired test, but it similarly did not find a significant difference.

```{r}
t.test(data$puzzle, data$video, paired=T, alternative='g')
wilcox.test(data$puzzle, data$video, paired=T, alternative='g')
```

## Exercise 2

```{r, include=FALSE}
data <- read.table("hemoglobin-1.txt", header=T)
data$rate <- as.factor(data$rate)
data$method <- as.factor(data$method)
```

**a)** We create a factorial design and distribute $N$ random fishes to each combination of the factors.

```{r, results = 'hide'}
I=4; J=2; N=10
data.frame(fish=sample(1:(N*I*J)), rate=rep(1:I, each=N*J), method=rep(1:J, N*I))
```


**b)** Here we first see that there is no interaction between the two factors. Next, we see that **method** does not have a significant impact on hemoglobin levels at all, while **rate** does.

```{r}
anova(lm(hemoglobin~rate*method, data=data))
```


**c)** This is not a particularly good question with the previous ANOVA test because it is unclear how a factor may influence the dependent variable through the interaction. Once we remove the interaction and consider only the additive model, we see that indeed **rate** has the greater impact and that **method** is not significant at all. Rate 2 leads to the significantly highest hemoglobin levels. Method B may be a little larger than method A but this cannot be ascertained with adequate statistical power. Therefore the highest mean hemoglobin combination is with rate 2 and probably method B.

```{r}
anova(lm(hemoglobin~rate+method, data=data))
summary(lm(hemoglobin~rate+method, data=data))
```

Below can be seen the mean hemoglobin when using rate 3 and method A. Also, as seen before, rate 2 leads to the highest hemoglobin values.

```{r}
mean(data$hemoglobin[data$rate==3 & data$method=='A'])
which.max(aggregate(hemoglobin ~ rate, data=data, mean)$hemoglobin)
```


**d)** The one-way ANOVA test shows that there is a difference in some means among the **rate** factor levels. In this dataset and after our initial analysis, we suspect that **method** does not influence the hemoglobin. Therefore, a block design is not warranted, and the one-way ANOVA is no more wrong or less useful than the two-way ANOVA.

```{r}
hemoaov = lm(hemoglobin ~ rate, data=data); anova(hemoaov)

# Point estimates for hemoglobin values for each rate
aggregate(hemoglobin ~ rate, data=data, mean)
```


**e)** ANOVA makes several assumptions while the Kruskal-Wallis is the generalization of the Wilcoxon test. ANOVA assumes normally distributed data, equal variance for all levels of all factors and their combinations, and that the residuals follow a normal distribution. With the small sample size, it is hard to know whether these conditions are met. On the other hand, the non-parametric Kruskal-Wallis test can be employed regardless. In our case, it similarly rejects $H_0$.

```{r}
kruskal.test(hemoglobin ~ rate, data=data)
```

However, there are several things we can do to check if using ANOVA is appropriate before the need for the Kruskal-Wallis test. The first is to check if the variance is identical. It us unclear if the variance is identical here. We can also check the residuals. Plotting the residuals against the fitted parameters should yield no structure. In our case ANOVA is probably fine but if we need to be completely certain, the Kruskal-Wallis may be preferable.

```{r}
# Checking the variance
aggregate(hemoglobin ~ rate:method, data=data, sd)

# Checking the residuals
par(mfrow=c(1, 1)); plot(fitted(hemoaov), residuals(hemoaov))
```


## Exercise 3

```{r, include=FALSE}
library(lme4)
data <- read.table("cream-1.txt", header=T)
data$batch <- as.factor(data$batch)
data$position <- as.factor(data$position)
data$starter <- as.factor(data$starter)
```

**a)** We ran an additive ANOVA model and found that the **starter** indeed have a strong effect on the **acidity**. Out of the two blocks, the **position** had no effect but the **batch** did.

The following summary shows that there is no statistically significant difference in effects of starters 1 and 2. The only starter that seems to deviate from the rest and yield significantly higher acidity is starter 4.

```{r}
aciditylm = lm(acidity ~ batch + position + starter, data); summary(aciditylm)
```


**b)** Since position had no effect on acidity, we removed it. Our findings remain identical as in **a)**. Here we see that starter 4 has the biggest effect on **acidity**. From the summary, we can see that $\alpha_4$ is the only one that is positive, and the only one that differs significantly from $\alpha_1$. This shows not only that starter 4 has the strongest positive effect on the acidity of the sour cream, but also that the reference starter, starter 1, has the second strongest, though insignificantly different from the rest of the lower acidity starters.

```{r}
aciditylm = lm(acidity ~ starter + batch, data); anova(aciditylm)
summary(aciditylm)
```

**c)** Friedman test is also applicable here, as it is similar to 2-way ANOVA but does not assume normality in
the data distribution. Therefore, it can be seen like a non-parametric counterpart to 2-way ANOVA. Here it leads to the same result that the starter does indeed have an effect on the acidity.

```{r}
friedman.test(acidity ~ starter | batch, data)
```

**d)** We fitted two linear mixed effects models on our data. One with the **starter** factor as a fixed effect and one without it. Then we ran an ANOVA to see if the 2 models were different, and indeed they were, as the previous experiments would have suggested. Looking into `summary(lm1)` (not shown here), we found identical alphas for the starter, but we also saw how much variance the random effect factors explained. Finally we plotted the residuals of the fitted model to ensure that ANOVA was appropriate here.

```{r}
lm1 = lmer(acidity ~ starter + (1|batch) + (1|position), data)
lm2 = lmer(acidity ~ (1|batch) + (1|position), data)
anova(lm1, lm2)
plot(fitted(lm1), residuals(lm1))
```
